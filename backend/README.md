# backend

このディレクトリには、スクレイピング用のクローラーや、学習モデルなど、データを処理するためのコードを記載します。

## 技術的な説明

### 話者の識別と分離の流れ

主な手順として以下の通りです。

1. 文字起こしの生成

国会中継の音声を OpenAI の Whisper というツールを使って文字に変換します。この変換結果は CSV 形式で保存されます。

2. 参考音声の作成

スクレイピングで取得した国会中継データから、各話者の発言時間をもとに 10 秒間のモノラル音声を取り出します。これをもとに、話者の参考音声データを作成します。

3. 話者の識別

上記で取得した参考音声と文字起こしの各セグメント (CSV 内の start と end の範囲)を embedding（ベクトル表現に変換）し、それぞれのベクトル間のコサイン距離を計算します。この距離が一定のしきい値以下の場合、話者が一致するとみなされます。この方法により、具体的な話者を特定することができます。

4. 不明な話者のクラスタリング

「不明な話者」である各セグメントの音声を [speechbrain/spkrec-ecapa-voxceleb](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb)で embedding し、これらを高次元空間上のポイントとして DBSCAN クラスタリングを実施します。これにより、話者が異なるものの、音声特性が似ているセグメントを同一クラスタとしてグループ化します。

DBSCAN は密度ベースのクラスタリング手法で、半径 ε 以内にある近傍ポイント数がある値(MinPts)を超えていれば、そのポイントをコアポイントと見なし、同じクラスタに属させます。この手法により、不明な話者の中でも音声特性に基づき複数のクラスタ（話者）に分けることが可能となり、一度不明とされたセグメントも有効に利用することができます。これによって、ある程度手動でアサインすることもしやすくなります。

また、このプロジェクトのユースケース以外にもベクトルデータが活かせるように、Hugging Face で埋め込みデータを公開しています。(都度更新していきます)

https://huggingface.co/datasets/yutakobayashi/diet-members-voice-embeddings

# Special Thanks

このプロジェクトは以下のオープンソースプロジェクトに大きく支えられています。

- [Politylink](https://github.com/politylink)

スクレイピングと Whisper の部分は PolityLink さんを参考にさせて頂きました。

- [speechbrain/spkrec-ecapa-voxceleb](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb)

話者の埋め込みモデルとして活用させて頂いています。
